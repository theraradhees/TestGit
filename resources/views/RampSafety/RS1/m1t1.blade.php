
<style>
div.a {
  text-align: center;
} 
div.b {
  text-align: justify;
} 
#center {
    text-align: center
}

#justify {
    text-align: justify
}

h1 { 
    font-weight: bold
    font-size: 14px
    text-align:center
}
p {
    text-align: justify
    font-size: 12px

}
.thin-border {
    border-color: black;
    border-width: 3px;
    border-style: solid;
}

body {
  counter-reset: section;
}

h3::before {
  counter-increment: section; 
  content: "" counter(section) ": ";                                   of each h3 */
}
</style>

<div class="a">
<h1>Safety Concept</h1>
<hr>
</div>

<div class="b">
<p>The  module  initially  presents  basic  safety  concepts,  as  the  foundation  upon which to understand the need for implementation of SMS on the ramp area. The basic safety concept is that of a continuous loop as shown in fig.1.1. The course thereafter  outlines  the  progressive  implementation  and  maintenance of  safety within  ramp  area  through  safety  preparation  of  ramp  personnel  before they proceed and conduct their assignment within ramp area.
</p>
<p align="justify">Image Fig 1.1</p>
<br>
<h2 class="b">
    Standard, Rules, and Regulation
</h2>
<p class="b">
    Standard, Rules and Regulation related to safety and ramp activities:
    <ul>
        <li>ICAO Annex 14 Aerodrome;</li>
        <li> ICAO Doc. 9859 Safety management Manual</li>
        <li>ICAO Doc. 9683 Human Factor Training Manual;</li>
        <li>Civil Aviation Safety Regulation (CASR) Part 139 Aerodrome;</li>
        <li>Manual of Standard (MOS) Part 139 Aerodromes;</li>
        <li>Ministry Decree No. SKEP/140/VI/1999 Requirements and Procedures for the operation of the vehicle in airside.</li>
    </ul>
    <br>
<h2 class="b">
Connotation
</h2>
<p>Depending  on  the  perspective,  the  concept  of  safety  in  aviation  may  have different connotations, such as:</p>

<ul>
    <li>Zero accidents or serious incidents — a view widely held by the traveling public;</li>
    <li>Freedom from danger or risk; i.e. those factors which cause or are likely to cause harm; </li>
    <li>Attitudes of employees of aviation organizations towards unsafe acts and conditions;</li>
    <li>Error avoidance</li>
    <li>Regulatory compliance.</li>
</ul>
<br>
<p class="b">
    Whatever the connotation, they all have one underlying commonality: the possibility of absolute control Zero accidents, freedom from hazards, and so forth, convey the idea that it would be possible by design or intervention to bring under control, in aviation operational contexts, all variables that can precipitate bad or damaging outcomes. However, while the elimination of accidents and/or serious incidents and the achievement of absolute control are certainly desirable, they are unachievable goals in open and dynamic operational contexts. Hazards are integral components of aviation operational   contexts. Failures and operational errors will occur in   aviation, in spite of the best and most accomplished efforts to prevent them. No human activity or human-made system can be guaranteed to be absolutely free from hazards and operational errors.
</p>
<p>
    Safety is therefore a concept that must encompass relatives rather than absolutes, whereby safety risks arising from the consequences of hazards in operational contexts must be acceptable in an inherently safe system. The key issue still resides in control, but relative rather than absolute control. As long as safety risks and operational errors are kept under a reasonable degree of control, a system as open and dynamic as commercial civil aviation is considered to be safe. In other words, safety risks and operational errors that are controlled to a reasonable degree are acceptable in an inherently safe system.
</p>
<p>
    Safety is increasingly viewed as the outcome of the management of certain organizational processes, which have the objective of keeping the safety risks of the consequences of hazards in operational contexts under organizational control. Thus,safety is considered to have the following meaning.
</p>
<div class="a">
    <p class="thin-border" style="font-style: italic">
        <br>
        Safety:The state in which the possibility of harm to persons or of property damage is reduced to, and maintained at or below, an acceptable level through a continuing process of hazard identification and safety risk management.
        <br> 
         * * *
        
    </p>
</div>
<br>
<div class="b">
<h2 class="b">
Accident Causation
</h2>

<p>
    Industry-wide acceptance of the concept of the organizational accident was made possible by a simple, yet graphically powerful, model developed by Professor James Reason, which provided a means for understanding how aviation (or any other production system) operates successfully or drifts into failure. According to this model, accidents require the coming together of a number of enabling factors —each one necessary, but in itself not sufficient to breach system defenses. Because complex systems such as aviation are extremely well-defended by layers of defenses in-depth, single-point failures are rarely consequential in the aviation system. 
</p>
<p>
    Equipment failures or operational errors are never the cause of breaches in safety defenses, but rather the triggers. Breaches in safety defenses are a delayed consequence of decisions made at the highest levels of the system, which remain dormant until their effects or damaging potential are activated by specific sets of operational circumstances. Under such specific circumstances, human failures or active failures at the operational level act as triggers of latent conditions conducive to facilitating a breach of the system’s inherent safety defenses. In the concept advanced by the Reason model, all accidents include a combination of both active and latent conditions.
</p>
<p>
    Active failures are actions or in actions, including errors and violations, which have an immediate adverse effect. They are generally viewed, with the benefit of hindsight, as unsafe acts. Active failures are generally associated with front-line personnel (pilots, air traffic controllers, aircraft mechanical engineers, etc.) and may result in a damaging outcome. They hold the potential to penetrate the defenses put in place by the organization, regulatory authorities, etc.to protect the aviation system. 
</p>
<p>
    Active  failures  may  be  the  result  of  normal  errors,  or  they  may  result  from deviations from  prescribed procedures  and  practices.  The  Reason  model recognizes that there are many error-and violation–producing condition sin any operational context that may affect individual or team performance.
</p>
<p>
    Active failures by operational personnel take place in an  operational context which includes latent conditions. Latent conditions are conditions present in the system well before a damaging outcome is experienced, and made evident by local  triggering factors. The consequences of latent conditions may  remain dormant for a long time. Individually, these latent conditions are usually not perceived as harmful, since they are not perceived as being failures in the first place.
</p>
<p>
    Latent conditions become evident once the system’s defenses have been breached. These conditions are generally created by people far removed in time and space from the event. Front-line operational personnel inherit latent conditions in the system, such as those created by poor equipment or task design; conflicting goals (e.g. service that is on time versus safety); defective organizations (e.g. poor internal communications); or management decisions (e.g. deferral of a maintenance item). The perspective underlying the organizational accident aims to identify and mitigate these latent conditions on a system-wide basis, rather than by localized efforts to minimize active failures by individuals. Active failures are only symptoms of safety problems, not causes.
</p>
<p>
    Even in the best-run organizations, most latent conditions start with the decision-makers. These decision makers are subject to normal human biases and limitations, as well as to real constraints such as time, budgets, and politics. Since downsides in managerial decisions cannot always be prevented, steps must be taken to detect them and to reduce their adverse consequences.
</p>
<p>
    Decisions by line management may result in inadequate training, scheduling conflicts or neglect of workplace precautions. They may lead to inadequate knowledge and skills or inappropriate operating procedures. How well line management and the organization as a whole perform their functions sets the scene for error-or violation-producing conditions. For example: How effective is management with respect to setting attainable work goals, organizing tasks and resources, managing day-to-day affairs, and communicating internally and externally? The decisions made by company management and regulatory authorities are too often the consequence of inadequate resources. However, avoiding the initial cost of strengthening the safety of the system can facilitate the pathway to the organizational accident.
</p>
<p>
    Figure 1.2 portrays the Reason model in a way that assists in understanding the interplay of organizational and management factors (i.e. system factors) in accident causation. Various defenses are built deep into the aviation system to protect against fluctuations in human performance or decisions with a downside at all levels of the system (i.e. the front-line workplace, supervisory levels and senior management). Defenses are resources provided by the system to protect against the safety risks that organizations involved in production activities generate and must control. This model shows that while organizational factors, including management decisions, can create latent conditions that could lead to breaches in the system’s defenses, they also contribute to the robustness of the system’s defenses.
</p>
<p style="text-align: center"">Figure 1.2</p>
</div>
<br>
<h2>People, Context and Safety — Shell Model</h2>
<p>Aviation workplaces are multi-component, multi-feature, complex operational contexts. Their functions and performance involve complex relationships among their many components in order for the system to achieve its production goals.</p>
<p>To understand the human contribution to safety and to support the human operational performance necessary to achieve the system’s production goals, it is necessary to understand how human operational performance maybe affected by the various components and features of the operational context and the interrelationships between components, features and people.</p>
<p>A very simple example is presented in Figure 1.3. The caveman is representative of operational personnel, and the mission (or production goal of the system) is to deliver packages to the other side of the mountains. The different components and features of the operational context and their interaction with the caveman, and among themselves, will impact the safety and efficiency of the delivery of packages. Thus, the interaction of the caveman with the lions may have detrimental effects in such delivery, unless the caveman is properly equipped to deal with the lions.</p>
<p>Transiting  though  the  mountains  on  a  probably  circuitous  and  unpaved  road withoutFootgear will detract from efficient performance (delays in delivering the packages) and may lead to injuries, thereby raising safety concerns. Braving the possible  weather  without  rain  gear  is  also  a  source  of  potential  deficiencies  in safety and efficiency.</p>
<br>
<p><b> 1.3 A. People and Safety</b></p>
<br>
<p>It is thus evident that proper consideration and analysis of the operational context is a source of valuable information in order   to understand operational performance, to support it and to enhance it.</p>
<br>
<p style="text-align: center"><b>Figure 1.3 B. People and Safety</b></p>
<p style="text-align: center"><b>Figure 1.3C. People and Safety</b></p>
<br>
<p>The need to understand operational performance within the operational context it takes places in is further illustrated through another example in Figure 1.3D.</p>
<br>
<p style="text-align: center"><b>Figure 1.3D. Understanding Human Performance</b></p>
<br>
<p>In this case, the system’s production objective is the delivery of packages by runners between points A and B. It is a basic assumption in the design of the system that runners will follow the shortest route, which is represented by the straight line.</p>
<p>No investment is spared to optimally resource the system. The best available human resources, in this case the runners, are selected, trained, indoctrinated and equipped with the best available running gear (technology). AS part of the system design, monitoring of operations in real time is included. Once design steps have been completed, operations begin. Shortly after system operational deployment, monitoring of operations in real time begins. Much to the dismay of system managers, real-time monitoring discloses that most runners do not follow the intended path, along the straight line, but rather a zigzagging path. As a consequence, delays in delivery take place, and also incidents occur(Figure 1.3E).</p>
<p>At this point, system managers have two options. One option is to follow the traditional perspective discussed produces hollow reminders to runners to do what they know and have been trained to do and allocate blame and punish the runners for failing to perform as expected. The other option is to analyze the operational context to see if there are components and features of the context that might be the source of adverse interactions with the runners. In following the second option, valuable information about certain components and features within the context will be acquired (Figure 1.3F), which will allow for the readjustment of design assumptions and the development of mitigation strategies for the safety risks of the consequences of unforeseen components and features of the context.In other words, by acquiring information on hazards in the operational context and understanding their interactions with people, system managers can bring the system back under organizational control.</p>
<br>
<p style="text-align: center"><b>Figure 1.3F. Understanding Human Performance</b></p>
<br>
<p>it is thus proposed that a proper understanding of operational performance and operational errors cannot be achieved without a proper understanding of the operational context in which operational performance and errors take place. This understanding cannot be achieved unless a clear differentiation is made between processes and outcomes. There is a tendency to allocate symmetry to causes and consequences of operational errors which, in real practice, does not exist. The very same error can have significantly different consequences, depending upon the context in which the operational error takes place.</p>
<p>The consequences of operational errors are not person-dependent but context dependent(Figure 1.4). This concept has a significant impact in mitigation strategies: efficient and effective error mitigation strategies aim at changing those features and components of the operational context that magnify the consequences of errors, rather than changing people.</p>
<br>
<p><b>Figure 1.4 Processes and outcomes</b></p>
<br>
<p>
    Figure 1.4also illustrates a scenario where the two managerial options discussed might apply. Following the traditional approach would lead to reminders about being careful when leaning (or not to lean) on windowsills and the dangers of pushing flowerpots out of the window, the re-writing of procedures to the previous effects, or punishment for pushing flowerpots out of the window (failure to perform as expected or to perform safely). On the other hand, the organizational approach would lead to installing containment net under the window, broadening the windowsill, using flowerpots of the frangible type, re-routing traffic under the window or, in extreme circumstances, fencing off the window. The bottom line is that by removing or modifying the error-inducing features of the operational context, an exponential reduction in the probability and severity of the consequences of operational errors is achieved.
</p>
<p>
    A simple, yet visually powerful, conceptual tool for the analysis of the components and features of operational contexts and their possible interactions with people is the SHEL model. The SHEL model (sometimes referred to as the SHEL(L) model) can be used to help visualize the interrelationships among the various components and features of the aviation system. This model places emphasis on the individual and the human’s interfaces with the other components and features of the aviation system. The SHEL model’s name is derived from the initial letters of its four components: 
</p>
<ul>
    <li>Software(S) (procedures, training, support, etc.);</li>
    <li>Hardware (H) (machines and equipment);</li>
    <li>Environment (E) (the operating circumstances in which the rest of the L-H-S system must function);and</li>
    <li>Liveware (L) (humans in the workplace).</li>
</ul>

<p>Figure 1.5depicts the SHEL model. This building-block diagram is intended to provide a basic understanding of the relationship of individuals to components and features in the workplace.</p>
<br>
<p style="text-align: center"><b>Figure 1.5. SHEL (L) Model</b></p>
<br>
<p>
    Liveware. In the center of the SHEL model are the humans at the front line of operations. Although humans are remarkably adaptable, they are subject to considerable variations in performance. Humans are not standardized to the same degree as hardware, so the edges of this block are not simple and straight. Humans do not interface perfectly with the various components of the world in which they work. To avoid tensions that may compromise human performance, the effects of irregularities at the interfaces between the various SHEL blocks and the central Liveware block must be understood. The other components of the system must be carefully matched to humans if stresses in the system are to be avoided.
</p>
<p>Several different factors put the rough edges on the Liveware block. Some of the more important factors affecting individual performance are listed below:</p>
<ul>
    <li>Physical factors. These include the human’s physical capabilities to perform the required tasks, e.g. strength, height, reach, vision and hearing.</li>
    <li>Physiological factors. These include those factors which affect the human’s internal physical processes, which can compromise physical and cognitive performance, e.g. oxygen availability, general health and fitness, disease or  illness,  tobacco,  drug  or  alcohol  use,  personal  stress, fatigue  and pregnancy.</li>
    <li>Psychological    factors. These include those factors affecting the psychological preparedness of the human to meet  all the circumstances that might occur, e.g. adequacy of training, knowledge and experience, and workload.</li>
    <li>Psycho-social factors. These include all those external factors in the social system of humans that bring pressure to bear on them in their work and non-work environments, e.g. an argument with a supervisor, labour-management disputes, a death in the family, personal financial problems or other domestic tension.</li>
</ul>
<p>
    The SHEL model is particularly useful in visualizing the interfaces between the various components of the aviation system. These include:
</p>
<ul>
    <li>
        Liveware-Hardware (L-H). The interface between the human and technology is the one most commonly considered when speaking of human performance. It determines how the human interfaces with the physical work environment, e.g. the design of seats to fit the sitting characteristics of the human body, displays to match the sensory and information processing characteristics of the user, and proper movement, coding and location of controls for the user. However, there is a natural human tendency to adapt to L-H mismatches. This tendency may mask serious deficiencies, which may only become evident after an occurrence.
    </li>
    <li>
        Liveware-Software (L-S). The L-S interface is the relationship between the human and the supporting systems found in the workplace, e.g. regulations, manuals, checklists, publications, standard operating procedures(SOPs) and computer software. It includes such “user-friendliness” issues as currency, accuracy, format and presentation, vocabulary, clarity and symbology.
    </li>
    <li>
        Liveware-Liveware (L-L). The L-L interface is the relationship between the human and other persons in the workplace. Flight crews, air traffic controllers, aircraft maintenance engineers and other operational personnel function as groups, and group influences play a role in determining human performance. The advent of crew resource management (CRM) has resulted in considerable focus on this interface. CRM training and its extension to air traffic services (ATS) (team resource management (TRM)) and maintenance(maintenance resource management (MRM)) focus on the management of operational errors. Staff/management relationships are also within the scope of this interface, as are corporate culture, corporate climate and company operating pressures, which can all significantly affect human performance.
    </li>
    <li>
        Liveware-Environment (L-E). This interface involves the relationship between the human and both the internal and external environments. The internal workplace environment includes such physical considerations as temperature, ambient light, noise, vibration and air quality. The external environment includes such things as visibility, turbulence and terrain. The twenty-four hour a day, seven days a week, aviation work environment includes disturbances to normal biological rhythms,e.g. sleep patterns. In addition, the aviation system operates within a context of broad political economic constraints, which in turn affect the overall corporate environment. Included here are such factors as the adequacy of physical facilities and supporting infrastructure, the local financial situation, and regulatory effectiveness. Just as the immediate work environment may create pressures to take shortcuts, inadequate infrastructure support may also compromise the quality of decision-making.
    </li>
</ul>
<p>
    Care needs to be taken in order that operational errors do not “filter through the cracks” at the interfaces. For the most part, the rough edges of these interfaces can be managed, for example:
</p>
<ul>
    <li>
        The designer can ensure the performance reliability of the equipment under specified operating conditions.
    </li>
    <li>
        During the certification process, the regulatory authority can define realistic conditions under which the equipment may be used.
    </li>
    <li>
        The organization’s management can develop standard operations procedures (SOPs) and provide initial and recurrent training forth safe use of the equipment.
    </li>
    <li>
        Individual equipment operators can ensure their familiarity and confidence in using the equipment safely under all required operating conditions.
    </li>
</ul>
<br>

<h2>
    Errors and Violations
</h2>
<h3>
    Operational Errors
</h3>
<p>
    The growth the aviation industry has experienced over the last two decades would have been impossible had advanced technology not been available to support the increased demand for the delivery of services. In production intensive industries like modern aviation, technology is essential to satisfy requirements regarding the delivery of services. This is a fundamental point often overlooked in safety analyses. The introduction of technology does not primarily aim at improving safety; the introduction of technology primarily aims at satisfying the demand for the increase in the delivery of services, while maintaining existing margins of safety.
</p>
<p>
    Technology is thus introduced on a massive scale in an effort to satisfy production demands. One result of this mass introduction of technology aimed at improved service delivery is that the Liveware-Hardware interface of the SHEL model is overlooked, or not always considered to the extent that it should. As a consequence, technology that is not sufficiently developed may be introduced prematurely, leading to unexpected failures. 
</p>
<p>
    While the introduction of underdeveloped technology is an inevitable consequence of the needs of any mass production industry, its relevance to the management of safety cannot be disregarded. People on the front lines, such as operational personnel, need to interact daily with technology while performing their operational tasks in order to achieve the delivery of services. If the Hardware-Liveware interface is not properly considered during technology design, and if the operational consequences of the interactions between people and technology are overlooked, the result is obvious: operational errors.
</p>
<p>
    The perspective of operational errors as an emerging property of human/technology systems brings a significantly different perspective to the management of safety when compared with the traditional, psychology-based perspective on operational errors. According to the psychology-based perspective, the source of error “resides” within the person, and is a consequence of specific psycho-social mechanisms explored and explained by the different branches of research and applied psychology.
</p>
<p>
    Attempting to anticipate and mitigate operational errors effectively following a psychology-based perspectives extremely difficult if not altogether impossible. Selection may filter out individuals without the basic traits needed for the job at hand, and behavior can be influenced by training and regulation. Nevertheless, the flaw of this perspective, from a strictly operational viewpoint, is clear: it is impossible to anticipate in a systematic manner typical human frailties such as distraction, tiredness and forgetfulness, and how they can interact with components and features of an operational context under specific operational conditions. Individual-based mitigation strategies are considered “soft” mitigations, because deficiencies in human performance will pop up when least expected, not necessarily in demanding situations, and unleash their damaging potential.
</p>
<p>
    The perspective of operational errors as an emerging property of human/technology systems removes the source of the operational error from the human and places it squarely in the physical world, in the L/H interface. A mismatching this interface is the source of the operational error. As part of the physical world, the source of the operational error thus becomes visible, and it can be articulated in operational terms (a switch is partially hidden by a lever making it difficult to observe its correct position during night-time operations) as opposed to scientific terms (perceptual limitations).
</p>
<p>
    The source of the operational error can therefore be anticipated and mitigated through operational interventions. There is not much that safety management can achieve regarding human perceptual limitations, but there is an array of options available through safety management to counteract the consequences of a design that includes a partially hidden switch. 
</p>
<p>
    It is part and parcel of the aviation safety tradition to consider operational errors as a contributing factor in most aviation occurrences. This view, based on the psychology-based perspective discussed above, portrays operational errors as a form of behavior in which operational personnel willingly engage, as if operational personnel had a clear option between electing to commit an operational error or not and willingly engage in the first option. Furthermore, an operational error is considered indicative of substandard performance, flaws in character, lack of professionalism, absence of discipline and similar attributions that years of partial understanding of human performance have developed. While convenient to describe events and expedient to blame people, these attributions stop short of understanding and explaining operational errors. 
</p>
<p>
    Following the alternative perspective on operational errors discussed, by considering operational errors as an emerging property of human/technology systems, and by placing the source of errors in the mismatch in the L/H interface, it becomes obvious that even the most competent personnel can commit operational errors. Operational errors are then accepted as a normal component of any system where humans and technology interact, and not considered as some type of aberrant behavior. Errors can be viewed rather as a natural by-product of human-technology interactions during operational activities aimed at the delivery of services of any production system. Operational errors are accepted as a normal component of any system where humans and technology interact, and operational safety strategies are put into practice to control operational errors.
</p>
<p>
    Given the inevitability of mismatches in the interfaces of the SHEL in aviation operations, the scope for operational errors in aviation is enormous. Understanding how these mismatches can affect the average human at work is fundamental to safety management. Only then can effective measures be implemented to control the effects of operational errors on safety.
</p>
<p>
    It is a common misperception to establish a linear relationship between operational errors and both the immediacy and magnitude of their consequences. This misperception is discussed in terms of operational errors and the magnitude of their consequences. The discussion argues that there is no symmetry between operational errors and the magnitude of their potential consequences. It further argues that the magnitude of the consequences of operational errors is a function of the operational context in which errors take place, rather than a consequence of the errors themselves. The discussion is furthered hereunder in terms of operational errors and the immediacy of their consequences. 
</p>
<p>
    It is a statistical fact that in aviation millions of operational errors are made on a daily basis before a major safety breakdown occurs (Figure 1.6). Minor yearly fluctuations aside, industry statistics consistently propose an accident rate of less than one fatal accident per million departures for the last decade. To put it in different terms, commercial airline operations worldwide, once every million production cycles an operational error is committed that develops damaging potential strong enough to penetrate system defenses and generate a major safety breakdown. 
</p>
<p>
    Nevertheless, mismatches in the interfaces of the SHEL model generate tens of thousands of operational errors on a daily basis during the course of normal aviation operations. These operational errors, however, are trapped by the built in defenses of the aviation system, and their damaging potential is mitigated, thus preventing negative consequences. In other words, control of operational errors takes place on a daily basis through the effective performance of the aviation system defenses. 
</p>
<p>
    Simple operational scenario is presented to explain the asymmetry between operational errors and the immediacy of their consequences (Figure 1.7A). Following engine start-up, a flight crew omits to select the flaps to the appropriate take-off setting during the after-engines-start scan flow, as indicated in the standard operating procedures. 
</p>
<p>
    An operational error has therefore been made, but there are no immediate consequences. The operational error has penetrated the first layer of defense (SOPs, flight crew scan flow sequence following engine start), but its damaging potential is still dormant. There are no immediate consequences; the operational error just remains in the system, in latency. 
</p>
<br>
<p style="text-align: center"><b>Figure 1.6. Operational errors and safety — A non-linear relationship</b></p>
<br>
<p>
    The flight crew performs the after-engines-start checklist, but do not detect the incorrect flap setting, and the aircraft initiates taxiing for departure. A second opportunity is thus missed to recover from the consequences of the operational error, which continues to remain in the system, still harmless. Nevertheless, the system is now in a state of deviation or undesired state (i.e. aircraft taxiing for departure with an incorrect flap setting). The flight crew performs the taxiing checklist and the before take-off checklist. On both occasions, the incorrect flap setting is missed. Further opportunities to recover from the consequences of the operational error are missed. The operational error remains inconsequential, but the status of deviation, or the undesired state of the system, magnifies. 
</p>
<br>
<p style="text-align: center"><b>Figure 1.7A. Investigation of major breakdowns — once in a million flights </b></p>
<br>
<p>
    The flight crew starts the take-off roll, and the take-off warning configuration sounds. The flight crew does not identify the reason for the warning and continues the take-off roll. The operational error still remains inconsequential, but the system’s undesired state has now progressed to a state of amplification. The aircraft lifts off in incorrect flaps configuration. The system has now progressed to a state of degradation, but the undesired state can still conceivably be recovered by the flight crew. The aircraft cannot sustain flight because of the incorrect flap setting and crashes. It is only at that point, after breaching a considerable number of built-in system defenses, that the operational error develops its full damaging potential and becomes consequential. The system experiences a catastrophic breakdown. 
</p>
<p>
    Notice the relatively considerable time span between the commissions of the operational error by the flight crew and the materialization of its unrecoverable damaging potential. Notice also the number of opportunities to recover from the consequences of the operational error through defenses built into the system. This time span is the time that a system affords to control the consequences of operational errors, and it is commensurate with the depth and efficiency of system defenses. This is the time span throughout which the management of safety operates with considerable potential for success.
</p>
<br><p style="text-align: center"><b>Figure 1.7B. Safety management — on almost every flight </b></p>
<br>
<p>
    The more built-in defenses and layers of containment the system includes, and the more efficient their performance, the greater the possibilities are of controlling the consequences of operational errors. The reverse is true.
</p>
<p>
    From the point of view of this discussion, one conclusion is apparent: the scenario is unavoidably what most accident investigations would capture: unmanaged operational errors that lead to catastrophic system breakdowns. This is valuable information about human and systemic failures; information that portrays what failed, what did not work, what defenses did not perform as intended. While valuable as a baseline, this information is not enough to fully understand safety breakdowns and should be complemented by information from alternative sources.
</p>
<p>
    Consider a modified version of the scenario depicted in (Figure 1.11B). Notice that there are at least four obvious instances where defenses could have been triggered to contain the damaging potential of the initial operational error (omission to select take-off flaps in the after-engines-start flight crew scan flow): 
</p>
<ul>
    <li>The after start checklist;</li>
        <li>The taxiing checklist; </li>
        <li>The before-take-off checklist;</li>
        <li>The take-off configuration warning.</li>
</ul>
<p>
    There are other instances, not as obvious but nonetheless possible, where defenses could have been triggered: warnings by ramp personnel, warnings by flight crews in similar aircraft, warnings by ATC personnel, etc. 
</p>
<p>
    Effective performance of the defenses in any of these instances could have controlled the consequences of the initial operational error and restored the system to normal status. The damaging potential of the operational error could have been eliminated at each instance thus making, for practical purposes, the operational error disappear.
</p>
<p>
    The argument advanced here is that scenarios where operational errors induce catastrophic breakdowns are rare, while scenarios where operational errors induce system undesired states (deviation/degradation) are frequent. These scenarios capture information on what initially did not work, but mostly about what thereafter worked, including defenses that performed as designed. This is the type of information that the sources of safety information, alternative and complementary to the investigation of accidents, capture. The information from an accident investigation would certainly identify the four instances in which defenses should have been triggered, but it can in all likelihood only describe why they were not. 
</p>
<p>
    The additional sources of information under discussion would identify the instances in which defenses should have been triggered and describe why and how they were. 
</p>
<p>
    These sources characterize successes, and, thus, integrating the information from accidents with the information from these alternative sources provides for a more complete picture about specific safety concerns. Furthermore, because scenarios as the one described above are frequent, these alternative sources of safety information, if deployed, can provide a considerable volume of constant information, to complement the more sporadic information provided by accidents, thus allowing for a fuller understanding about the potential for safety breakdowns. The conclusion than can be drawn from this second scenario is that safety resiliency is not so much a question of error-free operational performance, but rather a question of effective operational error management.
</p>
<br>

<h3>Three Strategies to Control Operational Errors </h3>
<p>
    The three basic strategies to control operational errors are based upon the three basic defenses of the aviation system: technology, training and regulations (including procedures). 
</p>
<p>
    Reduction strategies intervene directly at the source of the operational error by reducing or eliminating the factors contributing to the operational error. Examples of reduction strategies include improving the access to aircraft components for maintenance, improving the lighting in which the task is to be performed, and reducing environmental distractions, i.e.: 
</p>
<ul>
    <li>
        Human-centered design; 
    </li>
    <li>Ergonomic factors; and</li>
    <li>Training</li>
</ul>
<p>
    Capturing strategies assume the operational error has already been made. The intent is to “capture” the operational error before any adverse consequences of the operational error are felt. Capturing strategies are different from reduction strategies in that they do not directly serve to eliminate the error, i.e.:
</p>
<ul>
    <li>checklists;</li>
    <li>Task Cards; and</li>
    <li>Flight Strips</li>
</ul>
<p>
    Tolerance strategies refer to the ability of a system to accept an operational error without serious consequences. An example of a measure to increase system tolerance to operational errors is the incorporation of multiple hydraulic or electrical systems on an aircraft to provide redundancy, or a structural inspection program that provides multiple opportunities to detect a fatigue crack before it reaches critical length, i.e.: 
</p>
<ul>
    <li>
        System redundancies; and
    </li>
    <li>
        Structural inspections. 
    </li>
</ul>
<p>
    Operational error management must not be limited to front-line personnel. The performance of front-line personnel is, as depicted by the SHEL model, influenced by organizational, regulatory and environmental factors. For example, organizational processes, such as inadequate communication, ambiguous procedures, unreasonable scheduling, insufficient resources and unrealistic budgeting constitute the breeding grounds for operational errors. As already discussed, all these are processes over which an organization must have a reasonable degree of direct control. 
</p>
<br>
<h3>Errors Versus Violations</h3>
<p>
    Thus far, the discussion in this section has focused on operational errors, which have been characterized as a normal component of any system where people and technology interact to achieve system production goals. The discussion will now focus on violations, which are quite different from operational errors. Both can lead to failure of the system and can result in high-consequence situations. A clear differentiation between, and understanding of, operational errors and violations are essential for the management of safety. 
</p>
<p>
    The fundamental difference between operational errors and violations lies in intent. While an error is unintentional, a violation is a deliberate act. People committing operational errors are trying to do the right thing, but for the many reasons discussed in previous paragraphs on operational errors, they fail to achieve their expectations. People committing violations, on the other hand, know that they are engaging in behavior that involves a deviation from established procedures, protocols, norms or practices, yet they persevere in the intent.
</p>
<p>
    For example, a controller allows an aircraft to descend through the level of a cruising aircraft when the DME distance between them is 18 NM, and this occurs in circumstances where the correct separation minimum is 20 NM. If the controller miscalculated the difference in the DME distances advised by the pilots, this would be an operational error. If the controller calculated the distance correctly, and allowed the descending aircraft to continue through the level of the cruising aircraft, knowing that the required separation minimum did not exist, this would be a violation.
</p>
<p>
    In aviation, most violations are the result of deficient or unrealistic procedures where people have developed workarounds to accomplish the task. Most stem from a genuine desire to do a good job. Seldom are they acts of negligence. There are two general types of violations: situational violations and routine violations. 
    
</p>
<p>
    Situational violations occur due to the particular factors that exist at the time, such as time pressure or high workload. In spite of knowing that a violation is being incurred, goal-orientation and mission achievement lead people to deviate from norms, in the belief that the deviation does not bear adverse consequences. 
</p>
<p>
    Routine violations are violations which have become “the normal way of doing business” within a work group. They occur when the work group has difficulty following established procedures in order to get the job done, because of practicality/workability issues, deficiencies in human-technology interface design and so forth, and informally devise and adopt “better” procedures, which eventually become routine. This is the notion of normalization of deviance discussed. Routine violations are seldom considered as such by a work group, because their objective is to get the job done. They are considered as

“optimizing” devices, since they aim at saving time and effort by simplifying a task (even if it involves cutting corners). 
</p>
<p>
    A third type of violation, which is often overlooked, is organization-induced violations, which can be viewed as an extension of routine violations. The full potential of the safety message that violations can convey can be understood only when considered against the demands imposed by the organization regarding the delivery of the services for which the organization was created. Figure 1.12 depicts the relationship between the two basic considerations an organization must weigh and balance in relation to the delivery of its services and when defining its organizational processes: system output and related safety risks.
</p>
<p>
    In any organization engaged in the delivery of services, system output and safety risks are intertwined. As demands for system output (i.e. delivery of services) increase, the safety risks associated with the delivery of services also increase, because of the increase in exposure. Therefore, as Figure 1.-8 illustrates, minimum system output correlates with the lowest safety risk, while maximum system output correlates with the highest safety risk. Continuous operation exposed to the highest safety risks is undesirable, not only from a safety standpoint but also from a financial standpoint. Thus, organizations weight desirable output and tolerable safety risk, and define a system output that is less than the maximum possible, but which correlates with a tolerable level of safety risk. In so doing, the organization defines its production objectives as a function of balancing acceptable output with acceptable safety risk. 
</p>
<p>
    One fundamental decision related to the process of defining production objectives (agreed on the basis of a balance between system output and safety risks) is the establishment of the defenses that the organization needs to develop in order to protect itself from the safety risks it will generate while producing. As already discussed, the three basic defenses of the aviation system are technology, training and regulations (including procedures). Therefore, when defining its production objectives, the organization also needs to define the tools (technology) necessary to safely and effectively achieve service delivery; how to foster the behavior the workforce must exhibit to safely and efficiently use the tools (training), and the set of norms and procedures that dictate workforce performance (regulations).
</p>
<br>
<p style="text-align: center"><b>Figure 1-8. Understanding violations</b></p>
<br>
<p>
    Thus, system output, level of safety risk and defenses converge to the point that defines the production objectives of the organization. They also depict the boundaries of what may be called the “safety space of the organization”. The safety space represents a protected zone, the zone within which the defenses that the organization has erected guarantee maximum resilience to the safety risks the organization will face while delivering the system output in terms of production objectives.
</p>
<p>
    The reason for the maximum resilience afforded by that safety space is that the defenses erected by the organization are commensurate with the planned system output, which in turn is commensurate with the tolerable safety risk. In other words, the resources allocated by the organization to protect are appropriate to and commensurate with the activities related to the delivery of services. This does not mean that the organization cannot experience an accident, since accidents are random events resulting from the concatenation of unforeseeable circumstances. It means that the organization has arrangements for the management of safety that guarantee an acceptable level of control of safety risks during the delivery of services, under foreseeable circumstances.
    Simply put, the organization has done the best it possibly can, safety-wise.
</p>
<p>
    Given the dynamic nature of aviation, aviation organizations may occasionally face transient, short-term demands for increased output (i.e. increased delivery of services) for brief periods of time, for example, seasonal variations in seat demands, specific circumstances such as a worldwide sporting event, and so forth. In order to maintain the safety zone intact, the organization should review and rearrange or modify its existing allocation of resources, and strengthen existing defenses to counteract the increased output and the ensuing increased level of safety risk.
</p>
<p>
    Aviation history, sadly, suggests otherwise. Too often, as the aftermath of safety breakdowns show, aviation organizations try to cope with short periods of increased system output by “stretching” defenses: resorting to overtime instead of hiring additional personnel, thus leading to increased workload and fatigue; using technology in “more efficient” ways instead of incorporating additional technology; “optimizing” procedures and resources without revising standard operating procedures and norms, and so forth. 
</p>
<p>
    What this stretching of defenses effectively does is it places the organization outside the safety space, first into the violation space and, ultimately, into the exceptional violation space. In other words, in order to deliver the increased output with the same resources, operational personnel must deviate from established processes by resorting to short cuts or workarounds sanctioned by the organization. Operational personnel do not elect to engage in such short cuts or workarounds, the organization does. The colloquial expression “giving a leg up to the company” eloquently describes the situation in which people are forced to engage in organization-sanctioned deviations to deliver a system output incommensurate with the resources allocated to such an end. Hard evidence that the organization has drifted into the violation space is generally provided by incidents. 
</p>
<p>
    A learning organization will then reassess its allocation of resources to expand its safety space in order to maintain the harmony between system output, tolerable safety risk and defenses or, if unable to expand its safety space, it will retract into the established safety space by reducing the system output. Some organizations will ignore the warnings provided by incidents, persist in their course of action, and thus inevitably drift into the exceptional violation space. An accident is then a likely outcome. 
</p>